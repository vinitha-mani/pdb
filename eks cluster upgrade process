You're right! Let me explain EKS cluster upgrades in a simpler way, starting from basics. I'll teach this as if I'm explaining to freshers.

First, let's understand what components make up an EKS cluster:

1. **Control Plane**: Managed by AWS (we don't touch the servers directly)
2. **Worker Nodes**: Our EC2 instances where applications run
3. **Add-ons**: Extra components like CoreDNS, kube-proxy, Amazon VPC CNI

Let's start with understanding our current setup:



```yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: my-training-cluster
  region: us-west-2
  version: '1.26'  # Starting version

nodeGroups:
  - name: ng-1
    instanceType: t3.large
    desiredCapacity: 3
    labels:
      role: worker
    
addons:
  - name: vpc-cni
    version: v1.12.0
  - name: coredns
    version: v1.8.7
  - name: kube-proxy
    version: v1.26.2

```

**Step 1: How to Check Current Versions**

Let's first check what versions we're running:
```bash
# Check cluster version
aws eks describe-cluster --name my-training-cluster --query "cluster.version" --output text

# Check node group versions
aws eks describe-nodegroup --cluster-name my-training-cluster --nodegroup-name ng-1

# Check add-on versions
aws eks describe-addon --cluster-name my-training-cluster --addon-name vpc-cni
aws eks describe-addon --cluster-name my-training-cluster --addon-name coredns
aws eks describe-addon --cluster-name my-training-cluster --addon-name kube-proxy
```

**Step 2: Understanding Release Notes**

Let me show you how to read release notes (using 1.26 to 1.27 as example):

1. Go to AWS EKS documentation website
2. Look for "Amazon EKS Kubernetes Release Notes"
3. Important sections to focus on:



```markdown
# EKS Release Notes Guide

## 1. Version Changes
Look for these sections:
- Kubernetes Version Update
- New Features
- Important Changes

## 2. Breaking Changes Example
```yaml
# OLD WAY (1.26)
apiVersion: networking.k8s.io/v1beta1
kind: Ingress

# NEW WAY (1.27)
apiVersion: networking.k8s.io/v1
kind: Ingress
```

## 3. Add-on Version Requirements
- Check minimum required versions for:
  - VPC CNI
  - CoreDNS
  - kube-proxy

## 4. Action Items Checklist
- [ ] Check deprecated APIs
- [ ] Verify addon compatibility
- [ ] Test applications in test cluster
- [ ] Update monitoring/logging if needed

```

**Step 3: Creating an Upgrade Runbook**

A runbook is like a step-by-step guide for your upgrade. Let's create one:



```markdown
# EKS Cluster Upgrade Runbook

## Pre-Upgrade Checks
```bash
# 1. Check cluster health
kubectl get nodes
kubectl get pods --all-namespaces

# 2. Check PDB (Pod Disruption Budgets)
kubectl get pdb --all-namespaces

# 3. Verify resource usage
kubectl top nodes
kubectl top pods --all-namespaces
```

## Backup Critical Resources
```bash
# Create backup directory
mkdir -p eks-backup-$(date +%F)

# Backup all resources
kubectl get all --all-namespaces -o yaml > eks-backup-$(date +%F)/all-resources.yaml

# Backup specific resources
kubectl get configmap -n kube-system -o yaml > eks-backup-$(date +%F)/configmap-kube-system.yaml
kubectl get secret -n kube-system -o yaml > eks-backup-$(date +%F)/secret-kube-system.yaml
```

## Update Process
1. Control Plane Update
2. Add-ons Update
3. Node Groups Update

## Rollback Plan
- Document previous versions
- Keep backup files
- List emergency contacts

```

**Step 4: The Actual Upgrade Process**

Let's create manifest files for our upgrade:



```yaml
# 1. Cluster Update Configuration
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: my-training-cluster
  region: us-west-2
  version: '1.27'  # Target version

---
# 2. Node Group Configuration
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: my-training-cluster
  region: us-west-2

managedNodeGroups:
  - name: ng-1-new
    instanceType: t3.large
    desiredCapacity: 3
    labels:
      role: worker
    version: '1.27'  # Match cluster version

```

Now, let's execute the upgrade step by step:

1. **Upgrade Control Plane:**
```bash
# Using eksctl
eksctl upgrade cluster --name=my-training-cluster --version=1.27 --approve

# Or using AWS CLI
aws eks update-cluster-version \
    --name my-training-cluster \
    --kubernetes-version 1.27
```

2. **Upgrade Add-ons:**
```bash
# Update VPC CNI
aws eks update-addon \
    --cluster-name my-training-cluster \
    --addon-name vpc-cni \
    --addon-version v1.12.6

# Update CoreDNS
aws eks update-addon \
    --cluster-name my-training-cluster \
    --addon-name coredns \
    --addon-version v1.9.3

# Update kube-proxy
aws eks update-addon \
    --cluster-name my-training-cluster \
    --addon-name kube-proxy \
    --addon-version v1.27.1-eksbuild.1
```

3. **Upgrade Worker Nodes:**

There are two approaches for worker nodes. Let's learn both:

A) Using eksctl (Easier):
```bash
# Update existing node group
eksctl upgrade nodegroup \
    --cluster=my-training-cluster \
    --name=ng-1 \
    --kubernetes-version=1.27
```

B) Manual migration (More control):
```bash
# 1. Create new node group
eksctl create nodegroup -f new-nodegroup.yaml

# 2. Drain old node group
kubectl drain node-name --ignore-daemonsets

# 3. Delete old node group
eksctl delete nodegroup --cluster=my-training-cluster --name=ng-1
```

**Step 5: Verification After Upgrade**

Let's create a verification script:



```bash
#!/bin/bash

echo "=== Checking Cluster Version ==="
kubectl version --short

echo "=== Checking Node Versions ==="
kubectl get nodes -o wide

echo "=== Checking Add-on Versions ==="
aws eks describe-addon --cluster-name my-training-cluster --addon-name vpc-cni
aws eks describe-addon --cluster-name my-training-cluster --addon-name coredns
aws eks describe-addon --cluster-name my-training-cluster --addon-name kube-proxy

echo "=== Checking Pod Health ==="
kubectl get pods --all-namespaces | grep -v "Running\|Completed"

echo "=== Checking System Components ==="
kubectl get pods -n kube-system

echo "=== Testing Basic Functionality ==="
# Deploy test pod
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
EOF

# Wait for pod to be ready
sleep 30

# Check pod status
kubectl get pod test-pod

```

**Common Questions You Might Get in Interview:**

1. **Why do we upgrade EKS clusters?**
   - Security updates
   - New features
   - Bug fixes
   - Stay within supported versions (AWS supports only last 4 versions)

2. **What order should we upgrade in?**
   - Control plane first
   - Add-ons next
   - Worker nodes last

3. **What can go wrong during upgrades?**
   - Application compatibility issues
   - Resource constraints during node rotation
   - Add-on version mismatches

4. **How to minimize downtime?**
   - Use PodDisruptionBudgets
   - Proper resource requests/limits
   - Rolling updates
   - Multiple node groups

Remember:
- Always test in a dev/staging environment first
- Keep good backups
- Document each step
- Have a rollback plan
- Monitor the upgrade process

Would you like me to explain any part in more detail or show more examples?
